<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>rayanon - Software</title><link href="/" rel="alternate"></link><link href="/feeds/software.atom.xml" rel="self"></link><id>/</id><updated>2017-03-25T04:08:00+05:30</updated><entry><title>Backup strategies on Postgres</title><link href="/postgres-backup.html" rel="alternate"></link><published>2017-03-25T04:08:00+05:30</published><updated>2017-03-25T04:08:00+05:30</updated><author><name>Anon Ray</name></author><id>tag:None,2017-03-25:/postgres-backup.html</id><summary type="html">&lt;p&gt;If you are using any database in production you should always have backups.
Servers can crash, disks can fail, data can get corrupted, people can make
mistakes; a lot of failures can happen with the production instance. Hence it
is extremely important to have backups on a production&amp;nbsp;system.&lt;/p&gt;
&lt;p&gt;In …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you are using any database in production you should always have backups.
Servers can crash, disks can fail, data can get corrupted, people can make
mistakes; a lot of failures can happen with the production instance. Hence it
is extremely important to have backups on a production&amp;nbsp;system.&lt;/p&gt;
&lt;p&gt;In this post I&amp;#8217;m going to discuss various backup options that are available on
&lt;a href="https://postgresql.org"&gt;PostgreSQL&lt;/a&gt; and the various ways one can configure taking backups of the
database in production. I&amp;#8217;ll also discuss in what situation what kind of
backups are more&amp;nbsp;useful.&lt;/p&gt;
&lt;p&gt;First, let&amp;#8217;s look at the various backup options available in&amp;nbsp;Postgres:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Logical&amp;nbsp;backups&lt;/li&gt;
&lt;li&gt;Physical&amp;nbsp;backups&lt;/li&gt;
&lt;li&gt;Continuous&amp;nbsp;backups&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let&amp;#8217;s look at each of these&amp;nbsp;below.&lt;/p&gt;
&lt;h3&gt;Logical&amp;nbsp;backups&lt;/h3&gt;
&lt;p&gt;Logical backups are dumps of &lt;span class="caps"&gt;SQL&lt;/span&gt; statements which describe the state of the
database. They can be obtained using a tool like &lt;code&gt;pg_dump&lt;/code&gt;. These dump files
are just &lt;span class="caps"&gt;SQL&lt;/span&gt; files. Hence, they can be imported to another postgres instance
using &lt;code&gt;psql&lt;/code&gt; normally. It is usually a good idea to take one logical backup
before altering schema or migrating a&amp;nbsp;database.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ pg_dump -U admin -h db.example.com mydatabase &amp;gt; dump.sql
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The file &lt;code&gt;dump.sql&lt;/code&gt; is your backup. To recover it, run this in another&amp;nbsp;database:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ psql -U admin -h db2.example.com mydatabase &amp;lt; dump.sql
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;code&gt;-U&lt;/code&gt; takes the postgres username. Port can also be specified via &lt;code&gt;-p&lt;/code&gt;.&lt;/p&gt;
&lt;h3&gt;Physical&amp;nbsp;backups&lt;/h3&gt;
&lt;p&gt;Physical backups are backups via the filesystem. Meaning they can be snapshots
of the filesystem itself, or they could be just copies of the Postgres
database&amp;#8217;s data directory. The entire database instance&amp;#8217;s state is stored in
Postgres&amp;#8217; data directory; which is usually &lt;code&gt;/var/lib/postgresql/&amp;lt;VERSION&amp;gt;/data&lt;/code&gt;
on *nix&amp;nbsp;machines.&lt;/p&gt;
&lt;p&gt;You can use &lt;code&gt;rsync&lt;/code&gt; to periodically take backup of this data directory, and
then store the backup&amp;nbsp;elsewhere.&lt;/p&gt;
&lt;p&gt;As an&amp;nbsp;example:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ tar czf mydb-pgdata-21022017-1803.tar.gz  /var/lib/postgresql/9.6/data
$ rsync --progress --partial --recursive mydb-pgdata-21022017-1803.tar.gz &lt;span class="se"&gt;\&lt;/span&gt;
    backup@backup-server.example.com:/vol/pgdata_backups/
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We first make a compressed (gzipped) tarball of the data directory, and then
use &lt;code&gt;rsync&lt;/code&gt; to copy the files to our backup&amp;nbsp;server.&lt;/p&gt;
&lt;h3&gt;Continuous&amp;nbsp;backups&lt;/h3&gt;
&lt;p&gt;Continuous backups are an advanced feature of Postgres. Postgres database has
this design where it first commits a transaction a file on the disk, called the
Write-ahead Log (or &lt;span class="caps"&gt;WAL&lt;/span&gt;), and waits for the data to be actually written to the
disk; and only then commits the transaction to the current database object.
This way even if a Postgres server crashes in the middle of a transaction, it
will never have inconsistent state and when it recovers it can replay the &lt;span class="caps"&gt;WAL&lt;/span&gt;
to arrive at the state it was when it&amp;nbsp;crashed.&lt;/p&gt;
&lt;p&gt;If you think about it, this amazing &lt;span class="caps"&gt;WAL&lt;/span&gt; design results in a uber-amazing backup
system. These &lt;span class="caps"&gt;WAL&lt;/span&gt; logs can simply copied or streamed over to another machine
where&amp;nbsp;-&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;it can be kept as a backup - and this enables us to recover a Postgres
   database in any point in time. Just ask Postgres replay to that point in the
   &lt;span class="caps"&gt;WAL&lt;/span&gt; where you fancy. This also called a Point-in-time recovery (&lt;span class="caps"&gt;PITR&lt;/span&gt;) in &lt;span class="caps"&gt;DB&lt;/span&gt;&amp;nbsp;world.&lt;/li&gt;
&lt;li&gt;it can be replayed to a secondary Postgres instance and then this instance
   can be used as a slave, a replicated instance, a backup or even a secondary
   read-only&amp;nbsp;instance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Using the &lt;span class="caps"&gt;WAL&lt;/span&gt;, a lot of interesting things can be done like streaming
replication, hot standby failover instance, read-only slaves and few more.
Maybe I will write about these interesing bits in a future&amp;nbsp;post.&lt;/p&gt;
&lt;p&gt;Configuring continous backups is little more involved and probably will require
its own post and hence I will not cover configuration of continous backups
here. You can search about it on the internet. But I can suggest using &lt;a href="https://github.com/wal-e/wal-e"&gt;&lt;span class="caps"&gt;WAL&lt;/span&gt;-E&lt;/a&gt;,
which is a fantastic tool that makes setting up continous backup very easy.
Once you install and configure &lt;span class="caps"&gt;WAL&lt;/span&gt;-E it can stream your backup files to S3,
Google bucket, Azure&amp;nbsp;etc.&lt;/p&gt;
&lt;h3&gt;But which backup to use&amp;nbsp;where?&lt;/h3&gt;
&lt;p&gt;Now, the natural next question is which backup method to use. Should someone
use all the possible methods, or sticking to one method is good&amp;nbsp;enough?&lt;/p&gt;
&lt;p&gt;It can be confusing or daunting to look at so many options. Hence, I&amp;#8217;m here to
nudge you in the right direction with the right ideas. But don&amp;#8217;t take my ideas
for granted. Do your own research, think through your problems, and come up
with your own&amp;nbsp;conclusion.&lt;/p&gt;
&lt;p&gt;Almost in all situations continuous backup makes sense. It offers a streaming,
continuous backup of your production database and it also offers
Point-in-time-recovery. &lt;span class="caps"&gt;PITR&lt;/span&gt; can be very valuable, because it let&amp;#8217;s you specify
a timestamp and recover to it. Although, setting up backup and recovery
mechanism in this method is little more complicated - as there can be few edge
cases, specially regarding the process you will adapt to do recovery. Hence,
this method requires a bit of planning and hence takes more effort overall. But
probably also pays the best in the long&amp;nbsp;run.&lt;/p&gt;
&lt;p&gt;Filesystem level backups are error-prone, there is no guarantee that you get a
consistent snapshot. What if the moment when you were taking the snapshot of
the underlying filesystem, the database decided to write some new data. There
is no way for you to control that. Also by moving around the filesystem for our
backups, we are essentially not leveraging what the database can do for us.
Hence, for most of the cases filesystem backups can be ignored. If you are
really paranoid and want to keep redundant backups, sure go ahead and setup a
cronjob to rsync your &lt;span class="caps"&gt;PGDATA&lt;/span&gt; directory somewhere&amp;nbsp;safe. &lt;/p&gt;
&lt;p&gt;Logical backups give you a logical snapshot of your database. Logical snapshots
are tremendously useful if you are migrating your database. Its always a good
idea when you are migrating a schema, that you take a logical backup before
migrating. It is also useful for debugging purposes. You can look at two
different logical dumps and compare them to figure out data discrepancies.
It&amp;#8217;s not a bad idea to add a cronjob which takes a &lt;code&gt;pg_dump&lt;/code&gt; periodically,
along with your continuous backup configuration. That way you have a logical
snapshot of your database and you have redundant backups. Win-win&amp;nbsp;situation!&lt;/p&gt;
&lt;p&gt;On closing note, another important aspect of database backup and recovery
process is to always validate and check that current backups are happening and
are working - i.e from existing backups database can be recovered. In the light
of the recent Gitlab incident this has become even more apparent, that systems
fail and assumptions don&amp;#8217;t hold true always. Do not rely on the fact that you
have setup the system correctly 2 years back. As an &lt;span class="caps"&gt;DBA&lt;/span&gt; or admin it is not just
enough to setup a backup system, one should regularly monitor the backups and
also perform periodic drills, where they take one of the latest backups and try
to recover from&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;Backup systems are generally long running systems and people usually forget
about them right after configuring them; because they do not need to touch that
system in a long time. But eventually the day comes when they have recover from
the backups, and they can only hope that their backups are safe and somehow
magically working even if they have gone throuhg system/software failures,
hardware failure, data corruption and what&amp;nbsp;not.&lt;/p&gt;
&lt;p&gt;Hence knowing the status of the backups can make the difference of a world to
the application because after all the most important part of any application is
the&amp;nbsp;database.&lt;/p&gt;</content><category term="postgres"></category><category term="backups"></category><category term="sysadmin"></category></entry></feed>